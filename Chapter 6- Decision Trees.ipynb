{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Able to perform regression and classification task and also give multiple outputs. These are fundamental component of Random Forests, one of the most powerful machine learning tools in use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Visualizing a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] #Petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize our tree using export_graphviz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=\"iris_tree.dot\",\n",
    "    feature_names=iris.feature_names[2:],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the command \"dot -Tpng iris_tree.dot -o iris_tree.png\" to change the .dot to a .png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root nodes at depth 0 indicate the entry point of the decision tree. A leaf node has no children and will return a class or the probability of belonging to multiple classes.\n",
    "\n",
    "One of decision trees many qualities is ease of visualization and they require little preperation. They do not require feature scaling or centering.\n",
    "\n",
    "A node's value attribute will give the number of training instances of each class. The gini attribute is the \"purity\" of the node (a \"pure\" node vill have a score of 0). The score is computed like so: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$G_i = 1 - \\sum_{k=1}^{n} p_{i, k}^2$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_{i, k}$ is the ratio of class _k_ to all the instances in the _i_<sup>th</sup> node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm we use to create our treee (CART algorithm) only produces binary trees. Other algorithms like ID3 can create trees with more than two children."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models that are easy to interpret and understand are known as _white box models_. Decision trees are a member of such models. These contrast with _black box models_ like neural networks which make predictions that are much more difficult to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Class Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees make predictions much like previously examined classifiers. It traverses the tree with the instance data and once it reaches a leaf, it ourputs the probability of belonging to a certain class or classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.90740741,  0.09259259]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability values would not change if we were to increase the petal width beyond 6cm even though it would appear to be iris-virginica instead of the predicted iris-versicolor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CART Training Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn uses the _Classifications and Regression Tree_ algorithm to train Decision Trees. The algorithm works by splitting the training data into two subsets based on a single feature _k_ and threshold _t<sub>k</sub>_. The algorithm finds the feature-threshold pair that produces the purest subsets (weighted by size). The CART cost function is defined like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(k, t_k) = \\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $G_{left/right}$ measures the purity of the left/right subset and $m_{left/right}$ measures the number of instances in each subset. The algorithm will stop recursing once the depth _max depth_ has been reached or there are no splits which will reduce subset impurity. Other control hyperparameters exist like min_samples_split, min_samples_leaf, min_weight_fraction_leaf, and max_leaf_nodes. \n",
    "\n",
    "Finding the optimal tree is an $O(\\exp(m))$ problem so we use a greedy solution to improve runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions is done in $O(\\log_2(m))$. Training time will usually compare against all features (unless the max_features hyperparameter is used) resulting in a training time of $O(n \\times m \\log_2(m))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Impurity or Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machin learning, entropy is used as an impurity measure: a set's entropy is zero when the set contains just one class. This is expressed as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H_i = - \\sum_{k=1, p_{i, k \\neq 0}}^{n} p_{i,k} \\log(p_{i,k})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing entropy over Gini impurity usually doesn't result in a large change. When the difference is large, Gini impurity will isolate frequent classes while entropy will create more balanced trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees make no assumptions about the data and are extrememly prone to overfitting to the training data. A model without specified parameters is called a _nonparametric model_. A _paramatric model_ has a limited degree of freedom and carries a reduced risk of overfitting and an increased risk of underfitting.\n",
    "\n",
    "We can reduce the risk of overfitting by using restricting the Decision Tree's freedom (regularizing). The regularization paramters depend on the specific algorithm used but one can almost always restrict the maximum depth.\n",
    "\n",
    "The scikit-learn DecisionTreeClassifier carries other parameters to regularize the tree: min<span>\\_</span>samples<span>\\_</span>split (specifies the number of samples required to split a node), min<span>\\_</span>samples<span>\\_</span>leaf (specifies number of samples a leaf node must have), min<span>\\_</span>weight<span>\\_</span>fraction<span>\\_</span>leaf (same as min<span>\\_</span>samples<span>\\_</span>leaf but expressed as a fraction of total number of weighted instances), and max<span>\\_</span>features (maximum number of features to be evaluated. Increasing min<span>\\_\\*</span> and decreasing max<span>\\_\\*</span> will regularize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a Decision Tree regressor like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of predicting a class, the tree will now predict a value equal to the average target value of every instance in the node. The CART algorithm will now attempt to minimise the MSE of the instances in each node. The CART cost function now looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(k, t_k) = \\frac{m_{left}}{m}MSE_{left} + \\frac{m_{right}}{m}MSE_{right}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $MSE_{node} = \\sum_{i \\in node}(\\hat{y}_{node} - y^{(i)})^2$ and $\\hat{y}_{node} = \\frac{1}{m_{node}} \\sum_{i \\in node}y^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees have some drawbacks. They usually create orthagonal decision boundaries and are vulnerable to rotated data. One can limit this with PCA (Chapter 8) which will better orient the data.\n",
    "\n",
    "More generally, Decision trees are extremely sensitive to small variations in the training data. It's even possible to get a completely different tree from the same training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
