{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Up and Running with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TensorFlow_ is a powerful OSS library well-suited to large scale macine learning tasks. It can run tasks in parallel across multiple CPUs and GPUs and train a network of with millions parameters, billions of instances, and millions of features. In short it's flexible, scalable, and production-ready. Here's some of its qualities:\n",
    "* Runs on Windows, Linux, macOS, iOS, and Android.\n",
    "* Uses a simple Python API called _TF.Learn_ allowing users to train a neaural net with just a few lines of code.\n",
    "* Extensible through plugins like Keras or Pretty Tensor\n",
    "* Uses highly efficient C++ implementations of many standard ML operations, with a focus on neural networks.\n",
    "* Provides several advanced optimization nodes to minimize cost functions. This is done through TensorFlow's _automatic differentiating_, it's ability to automatically compute gradients of defined cost functions.\n",
    "* _TensorBoard_, a visualization tool for viewing computational graphs, learning curves, etc.\n",
    "* Google's cloud computing API\n",
    "* Dedicated development community\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Your First Graph and Running It in a Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's code for a simple graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important thing to understand is this code doesn't actually compute anything: it just sets up a computation graph. To evaluate the graph we'll need to set up a TensorFlow _session_ and use it to initialize our variables and the function `f`. The session will take care of placing variables and operations onto _devices_ such as CPUs and GPUs and running them. Here's code that creates a session, initializes the variables, then evaluates `f` and closes the session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or written a different way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `with` block, the session is set as the default session. Calling `x.initializer.run()` is equivalent to calling `tf.get_default_session().run(x.initializer)`. Similaraly, `f.eval()` is equivalent to `tf.get_default_session().run(f)`. The session is also conveniently closed at the end of the with block.\n",
    "\n",
    "Instead of manually initializing each variable, we can use the `global_variables_initializer()` function to do it for us. It won't perform the initalization immidiately, but will instead create a node in our graph to do it when run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() #actually initialize the variables\n",
    "    result = f.eval()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Jupyter or the Python shell, it may be easier to create an InteractiveSession, which automatically sets itself as the default session. We won't need a `with` block, but we will need to manually close the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow programs are typically split into two parts: the first builds a computation graph (called the _construction phase_) and the second runs it (the _execution phase_). The construction phase builds the computation graph which usually represents an ML model and the computations required to train it. The execution phase runs a loop that evaluates a training step repeatedly, gradually improving the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any node created is automatically added to the default graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is usually fine, but we may want to use multiple independent graphs. We can do this by making a new Graph and temporarily making it the default graph inside a with block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "print(x2.graph is graph)\n",
    "print(x2.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: in Jupyter or the Python shell, running the same command multiple times can result in a graph which contains duplicate nodes. We can reset the graph by restarting the Jupyter kernel or Python shell, or we can use `tf.reset_default_graph()` method*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lifecycle of a Node Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow automatically identifies the dependencies of a node and evaluates those dependencies first, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval())\n",
    "    print(z.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the code defines a very simple graph. Then it detects that `y` depends on `x`, which depends on `w`, so it first evaluates `w`, then `x`, then `y`. Then it detects `z` depends on `x`, which depends on `w`, and evaluates `w`, then `x`, then `z`. TensorFlow does not reuse the result of the previous calculation of `x` and `w` and evaluates these variables twice.\n",
    "\n",
    "All node values are dropped between graph runs except variables, whose life is maintained from their initializer call until the session is closed.\n",
    "\n",
    "To efficiently evaluate `y` and `z` in the above example, we must ask TensorFlow to evaluate both `y` and `z` in just one graph run like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Warn: in single-process TensorFlow, multiple sessions do not share any state even when reusing the same graph. In distributed TensorFlow (chapter 12) variable state is stored on the servers, not sessions, so multiple sessions can share the same variables.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow operations (or _ops_) can take any number of inputs and return any number of outputs. _Source ops_ like constants and variables take no inputs. The inputs and outputs are multidimensional arrays called _tensors_ and have a type and shape like NumPy arrays. The Python API actually represents the values with NumPy ndarrays. They typically contain floats, but can also contain strings or other arbitrary byte arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will manipulate the California housing dataset from chapter 2. It starts by fetching the data, then it adds an extra input bias feature ($x_0 = 1$) to all training instances using NumPy. It then creates two TensorFlow nodes, `X` and `y`, to hold the data and target values. After this it uses some matrix operations to define `theta`. These functions (`transpose()`, `matmul()`, and `matrix_inverse()`) do not perform computations immidiately but are represented as nodes in the graph to be performed when the graph is ran. `theta` corresponds directly to $(\\hat{\\theta} = X^T \\cdot X)^{-1} \\cdot X^T \\cdot y$. Finally, the code creates a session and uses it to evaluate `theta`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main benefit of this code over NumPy is that the Normal Equation computation will automatically be done on the GPU if available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll implement Batch Gradient Descent (Chapter 4) in TensorFlow. We'll manually compute the gradients, then use TensorFlow's autodiff feature to allow TensorFlow to automatically compute the gradients before we use some built-in optimizers.\n",
    "\n",
    "*Warn: Remember to normalize the input feature vectors to drastically improve training time. This can be done in any of the frameworks we've used already. The code assumes we've already done this.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Computing the Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes before we get into the code:\n",
    "* The `random_uniform()` function creates a node in the graph that generates random values given it's shape and value range.\n",
    "* The `assign()` function assigns a new value to a variable. We use it below in the Gradient Descent step $\\theta^{(next step)} = \\theta - \\eta \\nabla_{\\theta}MSE(\\theta)$\n",
    "* The main loop executes the training ste `n_epoch` times, printing the MSE every 100 iterations. The MSE should go down every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE =  5.62933\n",
      "Epoch 100 MSE =  5.62883\n",
      "Epoch 200 MSE =  5.62833\n",
      "Epoch 300 MSE =  5.62781\n",
      "Epoch 400 MSE =  5.62732\n",
      "Epoch 500 MSE =  5.62682\n",
      "Epoch 600 MSE =  5.62632\n",
      "Epoch 700 MSE =  5.62581\n",
      "Epoch 800 MSE =  5.62531\n",
      "Epoch 900 MSE =  5.62481\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "#Scale housing data with norm found with numpy\n",
    "scaled_housing_data_plus_bias = housing_data_plus_bias / np.linalg.norm(housing_data_plus_bias)\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE = \", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although manually computing the gradient isn't necessarily hard for tasks like Linear Regression, this would be much more difficult and error-prone to perform on a neural network. We could use _symbolic differentiation_ to find the equations for the partial derivatives, but even then the code would likely not be very efficient. \n",
    "\n",
    "For example, take the function $f(x) = \\exp(\\exp(\\exp(x)))$. From calculus, $f^{\\prime}(x) = \\exp(x)\\times\\exp(\\exp(x))\\times\\exp(\\exp(\\exp(x)))$. Computing these seperately results in inefficent code which caluculates the exponential nine times. If we had a function that computed $\\exp(x)$, then $\\exp(\\exp(x))$, then $\\exp(\\exp(\\exp(x)))$, and returned all three, we could simply call it three times and be done.\n",
    "\n",
    "How about the case where our function we need to partially derive is instead defined by arbitrary code? Good luck finding the partial derivatives here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_func(a, b):\n",
    "    z = 0\n",
    "    for i in range(100):\n",
    "        z = a * np.cos(z + i) + z * np.sin(b - i)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily TensorFlow's autodiff feature does this work of automatically and efficiently computing these gradients for us. Just replace the `gradients =` line with the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradients = tf.gradients(mse, [theta][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients() function takes an op (in this case `mse`) and a list of variables (in this case just `theta`) and creates a list of ops (one for each variable) to compute the gradients on the op with regards to each variable. So the gradients node will compute the gradient vector for MSE with regards to theta.\n",
    "\n",
    "There are four ways to compute gradients automatically. TensorFlow uses reverse-mode autodiff which is efficient and accurate on a large number of inputs and a few outputs, a common case with neural networks. This can be done in $n_{outputs} + 1$ graph traversals.\n",
    "\n",
    "Technique | # of graph traversals | Accuracy | Supports arbitrary code | Comment\n",
    ":--- | :--- | :--- | :--- | :---\n",
    "Numerical differentiation | $n_{inputs} + 1$ | Low | Yes | Trivial to implement\n",
    "Symbolic differentiation | N/A | High | No | Builds a very different graph\n",
    "Forward-mode autodiff | $n_{inputs}$ | High | Yes | Uses dual numbers\n",
    "Reverse-mode autodiff | $n_{outputs} + 1$ | High | Yes | Implemented in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only does TensorFlow automatically compute the gradient, but it can further simplify ML work by providing a number of built-in optimizers (like the GradientDescentOptimizer). we can replace the `gradients =` and `training_ops =` line with the following lines and everything will work normally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use a different optimizer we can make a change that will looks something like this (the momentum optimizer converges much faster thatn Gradient Descent, see chapter 11):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding Data to the Training Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to implement Mini-batch Gradient Descent. To start, we need a way to replace `X` and `y` with a mini-batch at each iteration. We can do this by initializing placeholder nodes, which don't perform any computations and output the data we need at runtime. These are usually used in TensorFlow to during training and will throw an exception if it's value is never specified.\n",
    "\n",
    "We'll set these nodes up with the `placeholder()` function and specify the data type. If needed, we could optionally pass in a shape that needs strict enforcement. Passing `None` to the dimension parameter means any size will be accepted. In the following code, we create a placeholder node `A` and `B = A + 5`. When we evaluate `B`, we pass the `feed_dict` to. the `eval()` method that specifies a value for `A`. `A` must have rank 2 (2-dimensional) and there must be three columns. It can have any number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.  7.  8.]]\n",
      "[[  9.  10.  11.]\n",
      " [ 12.  13.  14.]]\n"
     ]
    }
   ],
   "source": [
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n",
    "    \n",
    "print(B_val_1)\n",
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: We can feed in the output of **any** operations, not just placeholders.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement Mini-batch Gradient Descent, we only need to tweak the code slightly by changing the definitions for `X` and `y` in the construction phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define the batch size and total number of batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the execution phase we'll fetch our batches sequentially and supply values to `X` and `y` via `feed_dict` when an evaluation node requires it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We would actually need data on the disk to run this\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    #load data from disk\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(batch_size):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: we don't need to pass in `X` or `y` when evaluating `theta` because it doesn't depend on either of them.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Restoring Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a model is trained, it is often beneficial to save the parameters and model to the disk to retrieve and use the model whenever we want. In the event of a crash or other interruption we can also continue use or training from the last checkpoint.\n",
    "\n",
    "We can do this quickly in TensorFlow with the use of a `Saver` node at the end of the contruction phase (after all variables are created). Then we can call it's `save()` method in the execution phase whenever we want. We'll need to pass in the session we want to save and the checkpoint file path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some code\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "#more code\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0: #checkpoint every 100 epochs:\n",
    "            save_path = saver.save(sess, \"/tmp/model_partial.ckpt\")\n",
    "        \n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restoring a model is just as easy: we still initialize a saver at the end of the construction phase but this time we call it's `restore()` method at the beginning of the execution phase. This replaces the variable initialization we do with the `init` node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variables are saved and restored under their own name by default, but we can optionally specify which variables to save/restore and what names to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver({\"weights\": theta})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Graph and Training Curves Using TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use TensorBoard to create interactive displays of things like training stats and our graph definition in our web browser. We can use this visualization to identify errors, find bottlenecks, etc.\n",
    "\n",
    "First we'll need to tweak our program to write our graph definition and some training stats to a log directory. This directory should change between every run, otherwise TensorBoard will merge stats and give us misleading visuals. We can do this quickly by adding a timestamp to the directory name. Here's what we'll add at the beginning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir=\"{}/run-{}/\".format(root_logdir,now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And at the end of the construction phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line creates a node to evaluate the MSE value and write it to a TensorBoard compatiable log string called a _summary_. The second line creates a a `FileWriter` to write the summaries to the log directory. The first parameter is the log directory while the second is the optional graph. The `FileWriter` automatically creates the directory if it doesn't exist and writes the graph definition in a binary log file called an _events file_.\n",
    "\n",
    "Next we need to update the execution phase to evaluate `mse_summary` node regularly. This will output a summary that we can write to the events file using the `file_writer`, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code\n",
    "for batch_index in range(n_batches):\n",
    "    X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "    if batch_index % 10 == 0:\n",
    "        summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        step = epoch * n_batches + batch_index\n",
    "        file_writer.add_summary(summary_str, step)\n",
    "    sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "# more code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Warn: logging data at every training step would significantly slow training.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll close the `FileWriter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the completed program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for some reason this doesn't work\n",
    "from datetime import datetime\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "#X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "#y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir=\"{}/run-{}/\".format(root_logdir,now)\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "n_epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    if housing.data.size < batch_size*(batch_index + 1):\n",
    "        X_batch = housing_data_plus_bias[batch_size*batch_index: batch_size*(batch_index + 1)]\n",
    "        y_batch = housing.target[batch_size*batch_index: batch_size*(batch_index + 1)].reshape(-1,1)\n",
    "    else:\n",
    "        X_batch = housing_data_plus_bias[batch_size*batch_index:]\n",
    "        y_batch = housing.target[batch_size*batch_index:].reshape(-1,1)\n",
    "   \n",
    "    return X_batch, y_batch\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            save_path = saver.save(sess, \"/tmp/model_partial.ckpt\")\n",
    "            \n",
    "        for batch_index in range(batch_size):\n",
    "                X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "                if batch_index % 10 == 0:\n",
    "                    summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    step = epoch * n_batches + batch_index\n",
    "                    file_writer.add_summary(summary_str, step)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/model_final.ckpt\")\n",
    "    \n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can open up our directory and type **`ls -l tf_logs/run*`** to list the contents of our log directory. Running the program above a second time will reveal a second log directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name scopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid clutter we can use _name scopes_ to group related nodes. We'll modify the previous code to add `error` and `mse` to the `\"loss\"` name scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope() as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name is now prefixed with `\"loss/\"` and will now appear in the `loss` namespace in TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to add the the output of two _rectified linear units_ (ReLU). These compute a linear function of the inputs and outputs the result if it's positive, and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_{w, b}(X) = \\max(X\\cdot w + b, 0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code does this but it's very repetitive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights1\")\n",
    "w2 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights2\")\n",
    "b1 = tf.Variable(0.0, name=\"bias1\")\n",
    "b2 = tf.Variable(0.0, name=\"bias2\")\n",
    "\n",
    "z1 = tf.add(tf.matmul(X, w1), b1, name=\"z1\")\n",
    "z2 = tf.add(tf.matmul(X, w2), b2, name=\"z2\")\n",
    "\n",
    "relu1 = tf.maximum(z1, 0., name=\"relu1\")\n",
    "relu2 = tf.maximum(z2, 0., name=\"relu2\")\n",
    "\n",
    "output = tf.add(relu1, relu2, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of code is error prone and hard to maintain. TensorFlow allows us to stay DRY (Don't Repeat Yourself): simply create a function to build a ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow will automatically checki if a name has been used before, and if it has will append an underscore follwed by an index to keep track of them. These series will appear as a collapsed list in TensorBoard. We can clean this further by adding the content of `relu()` inside a name scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\") as scope:\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, 0., name=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple option to share a variable between components in a graph is to create it first, then pass it as a parameter to the functions that need it. If we want to control the ReLU threshold using a shared `threshold` variable for all ReLUs, we can create it and pass it to the `relu()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(X, threshold):\n",
    "    with tf.name_scope(\"relu\") as scope:\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works fine, but when the number of parameters is large it becomes difficult to keep track of which parameters are needed where. Some people use a dictionary of values or a class to control this, and another option might be setting the shared variable as an attribute like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\") as scope:\n",
    "        if not hasattr(relu, \"threshold\"):\n",
    "            relu.threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow adds another option which may result in cleaner, more modular code: we can use the `get_variable()` funtion to create a variable if it doesn't exist or reuse it if it does. The desired behavior is controlled by an attribute of the `variable_scope()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a variable has already been created by `get_variable()` an exception will be raised, which prevents reusing variables by mistake. To reuse a variable we need to set the variable scope's `reuse` attribute to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\", reuse=True):\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will use the existing `\"relu/threshold\"` variable or raise an exception if it des not exist or was not created. We can also set this value by using the scope's `reuse_variables()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Warn: once `reuse` is `True`, it cannot be set back to `False` within the block. Variables defined withing the block will inherit this value. Only variables created with `get_variables()` can be reused this way.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our `relu()` function can access the threshold value without needing it to be passed as a parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\", reuse=True):\n",
    "        threshold = tf.get_variable(\"threshold\")\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "    \n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code first defines the `relu()` function then creates the `relu/threshold` variable and builds 5 ReLUs with the `relu()` function. The `relu()` function reuses `relu/threshold` variable and creates the other nodes. \n",
    "\n",
    "*Note: variables created with `get_variable()` are always named using the name of their `variable_scope` as a prefix. , but for all other nodes the variable scope acts like a new name scope.*\n",
    "\n",
    "We can move the `threshold` definition inside the `relu()` function such that it is initialized on the first call and reused thereafter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, threshold, name=\"max\")\n",
    "        \n",
    "    \n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus =[]\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope(\"relu\", reuse=(relu_index >= 1)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
